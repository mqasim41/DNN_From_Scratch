{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks and Backpropagation\n",
        "\n",
        "Deep Neural Networks (DNNs) are a type of artificial neural network (ANN) with multiple hidden layers between the input and output layers. These networks are capable of learning complex patterns and representations from data, making them suitable for a wide range of tasks such as image recognition, natural language processing, and speech recognition.\n",
        "\n",
        "## Structure of Deep Neural Networks\n",
        "\n",
        "A DNN consists of multiple layers of interconnected neurons, organized into three main types of layers:\n",
        "\n",
        "1. **Input Layer**: This layer consists of neurons that receive input data. Each neuron represents a feature or attribute of the input data.\n",
        "\n",
        "2. **Hidden Layers**: These layers are responsible for learning and extracting meaningful features from the input data. Deep neural networks have multiple hidden layers, hence the term \"deep\". Each hidden layer performs transformations on the input data using weighted connections and activation functions.\n",
        "\n",
        "3. **Output Layer**: The final layer of the network produces the output predictions. The number of neurons in this layer depends on the type of problem being solved. For binary classification tasks, there may be a single neuron with a sigmoid activation function, while for multi-class classification tasks, there may be multiple neurons with softmax activation.\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity to the network, enabling it to learn complex mappings between inputs and outputs. Some commonly used activation functions in DNNs include:\n",
        "\n",
        "1. **Sigmoid**: $f(z) = \\frac{1}{1 + e^{-z}}$\n",
        "2. **ReLU (Rectified Linear Unit)**: $f(z) = \\max(0, z)$\n",
        "3. **Tanh (Hyperbolic Tangent)**: $f(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$\n",
        "4. **Softmax**: $f(z)_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "Backpropagation is a key algorithm used to train deep neural networks. It involves computing the gradient of the loss function with respect to the network's parameters (weights and biases) and updating these parameters to minimize the loss. The process consists of two main steps:\n",
        "\n",
        "1. **Forward Pass**: During the forward pass, input data is fed through the network, and predictions are made. The output of each layer is computed using the input data, weights, biases, and activation functions.\n",
        "\n",
        "2. **Backward Pass**: In the backward pass, the gradient of the loss function with respect to each parameter in the network is computed using the chain rule of calculus. This gradient indicates how much the loss would change with a small change in the parameter. The gradients are then used to update the parameters using optimization algorithms such as gradient descent.\n",
        "\n",
        "## Backpropagation Formulas\n",
        "\n",
        "### Loss Function:\n",
        "Let $L$ denote the loss function, $y_i$ the true label, and $\\hat{y}_i$ the predicted probability for class $i$. For binary classification, the commonly used loss function is binary cross-entropy:\n",
        "\n",
        "$$ L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
        "\n",
        "For multi-class classification, cross-entropy loss or categorical cross-entropy is typically used:\n",
        "\n",
        "$$ L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij}) $$\n",
        "\n",
        "Where:\n",
        "- $N$ is the number of samples\n",
        "- $C$ is the number of classes\n",
        "\n",
        "### Gradient Calculation:\n",
        "The gradients of the loss function with respect to the parameters of the network are computed using the chain rule. For a parameter $w_{ij}$ in layer $l$, the gradient is given by:\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_{j}^{(l)}} \\frac{\\partial a_{j}^{(l)}}{\\partial z_{j}^{(l)}} \\frac{\\partial z_{j}^{(l)}}{\\partial w_{ij}^{(l)}} $$\n",
        "\n",
        "Where:\n",
        "- $a_{j}^{(l)}$ is the activation of neuron $j$ in layer $l$\n",
        "- $z_{j}^{(l)}$ is the weighted sum of inputs to neuron $j$ in layer $l$\n",
        "\n",
        "### Parameter Update:\n",
        "The parameters of the network (weights and biases) are updated using an optimization algorithm such as gradient descent. The update rule for a parameter $w_{ij}^{(l)}$ is given by:\n",
        "\n",
        "$$ w_{ij}^{(l)} = w_{ij}^{(l)} - \\alpha \\frac{\\partial L}{\\partial w_{ij}^{(l)}} $$\n",
        "\n",
        "Where:\n",
        "- $\\alpha$ is the learning rate\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Deep neural networks are powerful models capable of learning complex patterns from data. Backpropagation is a fundamental algorithm for training these networks, allowing them to learn from labeled data by adjusting their parameters to minimize a given loss function. Understanding the concepts and mathematics behind deep neural networks and backpropagation is essential for effectively designing and training neural network models for various tasks."
      ],
      "metadata": {
        "id": "lEPv6YT3-e7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bc9xkr9c9yMp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Computes the sigmoid activation function element-wise on an input array.\n",
        "\n",
        "    Args:\n",
        "    Z (numpy.ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output array with sigmoid activation applied element-wise.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Computes the ReLU (Rectified Linear Unit) activation function element-wise on an input array.\n",
        "\n",
        "    Args:\n",
        "    Z (numpy.ndarray): Input array.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Output array with ReLU activation applied element-wise.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def loss(y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy loss between predicted probabilities and ground truth labels.\n",
        "\n",
        "    Args:\n",
        "    y_hat (numpy.ndarray): Predicted probabilities.\n",
        "    y (numpy.ndarray): Ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "    float: Binary cross-entropy loss.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-8  # Small epsilon value to ensure numerical stability\n",
        "    loss = -np.mean(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "G3rl0RFcAGPl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    \"\"\"\n",
        "    Dense (fully connected) layer in a neural network.\n",
        "\n",
        "    Args:\n",
        "    in_features (int): Number of input features.\n",
        "    out_units (int): Number of output units/neurons.\n",
        "    g (str): Activation function type. Supported values are \"relu\" or \"sigmoid\".\n",
        "\n",
        "    Attributes:\n",
        "    W (numpy.ndarray): Weight matrix.\n",
        "    b (numpy.ndarray): Bias vector.\n",
        "    g (str): Activation function type.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_units, g):\n",
        "        \"\"\"\n",
        "        Initializes the DenseLayer with random weights and biases.\n",
        "\n",
        "        Args:\n",
        "        in_features (int): Number of input features.\n",
        "        out_units (int): Number of output units/neurons.\n",
        "        g (str): Activation function type. Supported values are \"relu\" or \"sigmoid\".\n",
        "        \"\"\"\n",
        "        self.W = np.random.randn(out_units, in_features) * np.sqrt(2.0 / (in_features + out_units))\n",
        "        self.b = np.zeros((out_units, 1))\n",
        "        self.g = g\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs forward propagation through the layer.\n",
        "\n",
        "        Args:\n",
        "        X (numpy.ndarray): Input data.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Output of the layer after activation function.\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.Z = np.dot(self.W, X) + self.b\n",
        "        self.m = X.shape[1]\n",
        "        if self.g == \"relu\":\n",
        "            self.A = relu(self.Z)\n",
        "        elif self.g == \"sigmoid\":\n",
        "            self.A = sigmoid(self.Z)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Performs backward propagation through the layer.\n",
        "\n",
        "        Args:\n",
        "        dA (numpy.ndarray): Gradient of the loss with respect to the layer's output.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Gradient of the loss with respect to the layer's input.\n",
        "        \"\"\"\n",
        "        if self.g == \"relu\":\n",
        "            self.dZ = np.array(dA, copy=True)\n",
        "            self.dZ[self.Z <= 0] = 0\n",
        "        elif self.g == \"sigmoid\":\n",
        "            s = 1 / (1 + np.exp(-self.Z))\n",
        "            self.dZ = dA * s * (1 - s)\n",
        "\n",
        "        self.dW = (1 / self.m) * np.dot(self.dZ, self.X.T)\n",
        "        self.db = (1 / self.m) * np.sum(self.dZ, axis=1, keepdims=True)\n",
        "        self.dA = np.dot(self.W.T, self.dZ)\n",
        "\n",
        "        return self.dA\n",
        "\n",
        "    def gradient_descent_step(self, alpha):\n",
        "        \"\"\"\n",
        "        Performs one step of gradient descent update.\n",
        "\n",
        "        Args:\n",
        "        alpha (float): Learning rate.\n",
        "\n",
        "        \"\"\"\n",
        "        self.W = self.W - alpha * self.dW\n",
        "        self.b = self.b - alpha * self.db"
      ],
      "metadata": {
        "id": "oJGPfnSUAoH3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NNSequential:\n",
        "    \"\"\"\n",
        "    Neural Network model trained using a sequential approach.\n",
        "\n",
        "    Args:\n",
        "    X (numpy.ndarray): Input data.\n",
        "    Y (numpy.ndarray): Ground truth labels.\n",
        "    alpha (float): Learning rate.\n",
        "    num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Attributes:\n",
        "    layers (list): List of layers in the neural network.\n",
        "    X (numpy.ndarray): Input data.\n",
        "    Y (numpy.ndarray): Ground truth labels.\n",
        "    learning_rate (float): Learning rate.\n",
        "    num_epochs (int): Number of training epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, Y, alpha, num_epochs):\n",
        "        \"\"\"\n",
        "        Initializes the NNSequential model.\n",
        "\n",
        "        Args:\n",
        "        X (numpy.ndarray): Input data.\n",
        "        Y (numpy.ndarray): Ground truth labels.\n",
        "        alpha (float): Learning rate.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        \"\"\"\n",
        "        self.layers = []\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.learning_rate = alpha\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        \"\"\"\n",
        "        Adds a layer to the neural network.\n",
        "\n",
        "        Args:\n",
        "        layer: Layer to be added to the neural network.\n",
        "        \"\"\"\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Trains the neural network model.\n",
        "        \"\"\"\n",
        "        if self.num_epochs <= 0:\n",
        "            print(\"Number of epochs should be a positive integer.\")\n",
        "            return\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            A = np.array(self.X, copy=True)\n",
        "            for layer in self.layers:\n",
        "                A = layer.forward(A)\n",
        "\n",
        "            dA = - (np.divide(self.Y, A) - np.divide(1 - self.Y, 1 - A))\n",
        "\n",
        "            for layer in reversed(self.layers):\n",
        "                dA_l_minus_1 = layer.backward(dA)\n",
        "                dA = dA_l_minus_1\n",
        "\n",
        "            for layer in self.layers:\n",
        "                layer.gradient_descent_step(self.learning_rate)\n",
        "\n",
        "            print(\"Loss: \" + str(loss(A, self.Y)))\n",
        "\n",
        "    def predict(self, X, y_test):\n",
        "        \"\"\"\n",
        "        Predicts the output for the given input data and evaluates performance metrics.\n",
        "\n",
        "        Args:\n",
        "        X (numpy.ndarray): Input data for prediction.\n",
        "        y_test (numpy.ndarray): Ground truth labels for evaluation.\n",
        "\n",
        "        Returns:\n",
        "        numpy.ndarray: Predicted output.\n",
        "        \"\"\"\n",
        "        A = np.array(X, copy=True)\n",
        "        for layer in self.layers:\n",
        "            A = layer.forward(A)\n",
        "\n",
        "        y_pred = np.where(A > 0.5, 1, 0)\n",
        "        print(\"Predicted Output Shape:\", y_pred.shape)\n",
        "\n",
        "        y_test = np.squeeze(y_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred.T)\n",
        "        print(\"Accuracy:\", accuracy)\n",
        "\n",
        "        f1 = f1_score(y_test, y_pred.T)\n",
        "        print(\"F1-score:\", f1)\n",
        "\n",
        "        precision = precision_score(y_test, y_pred.T)\n",
        "        print(\"Precision:\", precision)\n",
        "\n",
        "        recall = recall_score(y_test, y_pred.T)\n",
        "        print(\"Recall:\", recall)\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "Sn3f06_hHikb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
        "\n",
        "\n",
        "X = mnist.data\n",
        "y = mnist.target\n",
        "\n",
        "\n",
        "y = y.astype(int)\n",
        "print(\"Feature vectors shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50000, test_size=20000, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# Add bias column to X_train and X_test\n",
        "X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "print(\"X_train shape (with bias column):\", X_train.shape)\n",
        "print(\"X_test shape (with bias column):\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBMG-jleMN_E",
        "outputId": "addbf3c2-afe4-42cb-84ec-cd7c69acd58d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature vectors shape: (70000, 784)\n",
            "Labels shape: (70000,)\n",
            "X_train shape: (50000, 784)\n",
            "X_test shape: (20000, 784)\n",
            "y_train shape: (50000,)\n",
            "y_test shape: (20000,)\n",
            "X_train shape (with bias column): (50000, 785)\n",
            "X_test shape (with bias column): (20000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize X-train by dividing by 255, the max value of RGB pixels.\n",
        "X_train /= 255\n",
        "X_test  /= 255"
      ],
      "metadata": {
        "id": "eyrbitV4Myix"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_labels(y, value_to_one):\n",
        "    \"\"\"\n",
        "    Preprocesses the labels such that labels with the specified value are changed to 1\n",
        "    and all other labels are changed to 0.\n",
        "\n",
        "    Parameters:\n",
        "    - y: numpy array, the original labels\n",
        "    - value_to_one: int, the value to be changed to 1\n",
        "\n",
        "    Returns:\n",
        "    - y_processed: numpy array, the preprocessed labels\n",
        "    \"\"\"\n",
        "    y_processed = (y == value_to_one).astype(int)\n",
        "    return y_processed"
      ],
      "metadata": {
        "id": "tz-DxyCyMzqu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = preprocess_labels(y_train, 8)\n",
        "y_test = preprocess_labels(y_test, 8)"
      ],
      "metadata": {
        "id": "0TTAsF-dM6Cz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XrzurhmNnqe",
        "outputId": "2b513898-7382-48e2-df54-e4e25c7f20d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NNSequential(X_train.T, y_train, 0.3, 40)\n",
        "layer1 = DenseLayer(785, 395, \"relu\")\n",
        "layer2 = DenseLayer(395, 200, \"relu\")\n",
        "layer3 = DenseLayer(200, 100, \"relu\")\n",
        "layer4 = DenseLayer(100, 20, \"relu\")\n",
        "layer5 = DenseLayer(20, 1, \"sigmoid\")\n",
        "model.add_layer(layer1)\n",
        "model.add_layer(layer2)\n",
        "model.add_layer(layer3)\n",
        "model.add_layer(layer4)\n",
        "model.add_layer(layer5)"
      ],
      "metadata": {
        "id": "r-Zuo3kIMQbi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyCLBFQdOYpy",
        "outputId": "670e9821-176b-4c55-dbcd-70831989fd22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.6769232302749445\n",
            "Loss: 0.4731457871064196\n",
            "Loss: 0.36120666619088\n",
            "Loss: 0.3443704228772589\n",
            "Loss: 0.33554247036164087\n",
            "Loss: 0.3271241362197645\n",
            "Loss: 0.31883781204130346\n",
            "Loss: 0.3103004736751103\n",
            "Loss: 0.3013139984139406\n",
            "Loss: 0.29203153200114385\n",
            "Loss: 0.2824937125847092\n",
            "Loss: 0.2726961338102551\n",
            "Loss: 0.2627090661276295\n",
            "Loss: 0.25261691455129986\n",
            "Loss: 0.24250532601691568\n",
            "Loss: 0.2326800162332227\n",
            "Loss: 0.22353467228063736\n",
            "Loss: 0.2152557976410191\n",
            "Loss: 0.20776776367654515\n",
            "Loss: 0.2009515267289394\n",
            "Loss: 0.1946947736761549\n",
            "Loss: 0.18895410507198493\n",
            "Loss: 0.1837711487653117\n",
            "Loss: 0.1798973277343879\n",
            "Loss: 0.18186625482790017\n",
            "Loss: 0.2169254341888924\n",
            "Loss: 0.3985554292447169\n",
            "Loss: 0.4040889052465391\n",
            "Loss: 0.292943634882196\n",
            "Loss: 0.19782695083127114\n",
            "Loss: 0.18247006680760827\n",
            "Loss: 0.17480379675258048\n",
            "Loss: 0.1693602110531448\n",
            "Loss: 0.16498019102769504\n",
            "Loss: 0.16110813283047506\n",
            "Loss: 0.15752398145475038\n",
            "Loss: 0.1541039420430122\n",
            "Loss: 0.15081394730296374\n",
            "Loss: 0.14763324506769374\n",
            "Loss: 0.14459190208552256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test.T, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NhDPfda2FFD",
        "outputId": "bbb5b543-0039-43c9-fe68-39edeb64169b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Output Shape: (1, 20000)\n",
            "Accuracy: 0.95095\n",
            "F1-score: 0.6625386996904025\n",
            "Precision: 0.9678391959798995\n",
            "Recall: 0.5036610878661087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}